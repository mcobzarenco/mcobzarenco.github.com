---
layout: post
title: "The log-sum-exp trick"
date: 2011-04-29 15:24
comments: true
categories:
---

When dealing with Hidden Markov Models (HMMs), but in other cases as well, one often needs to compute quantities of the form:

$$ a = \log\sum_t e^{b_t} $$

with bi extremely small so that the above computation easily underflows. The trick also works when bt is large. For instance, in the classic problem of filtering, the posterior of ht is computed recursively:

p(ht|v1:t)≡α(ht)=p(vt|ht)∑htα(ht−1)p(ht|ht−1)
The alphas above can get very small and one way of guarding against underflow is to work in log-space:

logα(ht)=logp(vt|ht)+log∑htexp(logα(ht−1)+logp(ht|ht−1))
Generically,

logsumexp({at})=log∑teat
And

log∑teat=log∑teateA−A=A+log∑teat−A
Where A=max{at}. The overhead paid for extra numerical accuracy is one maximisation operation and an extra subtraction for each element of the vector.
